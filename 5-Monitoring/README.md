# 5. Model Monitoring

- 5.1 [Intro to ML monitoring](#1-ml-monitoring-intro)
- 5.2 [Environment setup](#2-env-setup)
- 5.3 [Prepare reference and model](#3-prepare)
- 5.4 [Evidently metrics calculation](#4-metrics-calc)
- 5.5 [Evidently Monitoring Dashboard](#5-dashboard)
- 5.6 [Dummy monitoring](#6-dummy)
- 5.7 [Data quality monitoring](#7-quality-monitoring)
- 5.8 [Save Grafana Dashboard](#8-grafana-dashboard)
- 5.9 [Debugging with test suites and reports](#9-debugging)
- 5.10 [Homework](#10-homework)
 

<a id="1-ml-monitoring-intro"></a>
## 5.1 Intro to ML monitoring

- How to monitor ML models in production
- Avoiding degrading of models by monitoring them and taking appropriate steps

How is ML monitoring different to "regular" monitoring? It adds an additional layer of "stuff" to be monitored!
- **Service Health (a must)**: Uptime, Memory, Latency
- **Model Health and Data Health**: Model accuracy, data drift, concept drift, broken pipelines, schema change, data outage, model bias, underperforming segments

A checklist (what requires what monitoring):
1. **Service health**: Does it work 
2. **Model performance**: How it performs? / Did anything break?
3. **Data quality and integrity**: Where it breaks? / Where to dig further?
4. **Data and concept drift**: 

<a id="2-env-setup"></a>
## 5.2 Environment setup


<a id="3-prepare"></a>
## 5.3 Prepare reference and model


<a id="4-metrics-calc"></a>
## 5.4 Evidently metrics calculation


<a id="5-dashboard"></a>
## 5.5 Evidently Monitoring Dashboard


<a id="6-dummy"></a>
## 5.6 Dummy monitoring


<a id="7-quality-monitoring"></a>
## 5.7 Data quality monitoring

<a id="8-grafana-dashboard"></a>
## 5.8 Save Grafana Dashboard

<a id="9-debugging"></a>
## 5.9 Debugging with test suites and reports

<a id="10-homework"></a>
## 5.10 Homework